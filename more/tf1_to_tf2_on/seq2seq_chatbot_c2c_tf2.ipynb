{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DF0l_L8VXAwp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os, re\n",
        "from tensorflow.python.layers.core import Dense\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_CHAR_PER_LINE = 20\n",
        "\n",
        "def load_sentences(path):\n",
        "    with open(path, 'r', encoding=\"ISO-8859-1\") as f:\n",
        "        data_raw = f.read().encode('ascii', 'ignore').decode('UTF-8').lower()\n",
        "        data_alpha = re.sub('[^a-z\\n]+', ' ', data_raw)\n",
        "        data = []\n",
        "        for line in data_alpha.split('\\n'):\n",
        "            data.append(line[:MAX_CHAR_PER_LINE])\n",
        "    return data\n",
        "\n",
        "def extract_character_vocab(data):\n",
        "    special_symbols = ['', '', '',  '']\n",
        "    set_symbols = set([character for line in data for character in line])\n",
        "    all_symbols = special_symbols + list(set_symbols)\n",
        "    int_to_symbol = {word_i: word for word_i, word in enumerate(all_symbols)}\n",
        "    symbol_to_int = {word: word_i for word_i, word in int_to_symbol.items()}\n",
        "    return int_to_symbol, symbol_to_int\n",
        "\n",
        "input_sentences = load_sentences('data/words_input.txt')  \n",
        "output_sentences = load_sentences('data/words_output.txt')  \n",
        "\n",
        "input_int_to_symbol, input_symbol_to_int = extract_character_vocab(input_sentences)\n",
        "output_int_to_symbol, output_symbol_to_int = extract_character_vocab(output_sentences)"
      ],
      "metadata": {
        "id": "xynlGA8vXGyP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCS = 300\n",
        "RNN_STATE_DIM = 512\n",
        "RNN_NUM_LAYERS = 2\n",
        "ENCODER_EMBEDDING_DIM = DECODER_EMBEDDING_DIM = 64\n",
        " \n",
        "BATCH_SIZE = int(32)\n",
        "LEARNING_RATE = 0.0003\n",
        " \n",
        "INPUT_NUM_VOCAB = len(input_symbol_to_int)\n",
        "OUTPUT_NUM_VOCAB = len(output_symbol_to_int)"
      ],
      "metadata": {
        "id": "MZFcJCosXTRw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder placeholders\n",
        "encoder_input_seq = tf.compat.v1.placeholder(\n",
        "    tf.int32, \n",
        "    [None, None],  \n",
        "    name='encoder_input_seq'\n",
        ")\n",
        "\n",
        "encoder_seq_len = tf.compat.v1.placeholder(\n",
        "    tf.int32, \n",
        "    (None,), \n",
        "    name='encoder_seq_len'\n",
        ")\n",
        " \n",
        "# Decoder placeholders\n",
        "decoder_output_seq = tf.compat.v1.placeholder( \n",
        "    tf.int32, \n",
        "    [None, None],\n",
        "    name='decoder_output_seq'\n",
        ")\n",
        "\n",
        "decoder_seq_len = tf.compat.v1.placeholder(\n",
        "    tf.int32,\n",
        "    (None,), \n",
        "    name='decoder_seq_len'\n",
        ")\n",
        "\n",
        "max_decoder_seq_len = tf.reduce_max( \n",
        "    input_tensor=decoder_seq_len, \n",
        "    name='max_decoder_seq_len'\n",
        ")\n"
      ],
      "metadata": {
        "id": "s7UZ_ptkXZAC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_cell(state_dim):\n",
        "    lstm_initializer = tf.compat.v1.random_uniform_initializer(-0.1, 0.1)\n",
        "    return tf.compat.v1.nn.rnn_cell.LSTMCell(state_dim, initializer=lstm_initializer)\n",
        " \n",
        "def make_multi_cell(state_dim, num_layers):\n",
        "    cells = [make_cell(state_dim) for _ in range(num_layers)]\n",
        "    return tf.compat.v1.nn.rnn_cell.MultiRNNCell(cells)\n"
      ],
      "metadata": {
        "id": "lIgAruW9Xfht"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder embedding\n",
        " \n",
        "encoder_input_embedded = tf.contrib.layers.embed_sequence(\n",
        "    encoder_input_seq,     \n",
        "    INPUT_NUM_VOCAB,        \n",
        "    ENCODER_EMBEDDING_DIM  \n",
        ")\n",
        " \n",
        " \n",
        "# Encodering output\n",
        " \n",
        "encoder_multi_cell = make_multi_cell(RNN_STATE_DIM, RNN_NUM_LAYERS)\n",
        " \n",
        "encoder_output, encoder_state = tf.compat.v1.nn.dynamic_rnn(\n",
        "    encoder_multi_cell, \n",
        "    encoder_input_embedded, \n",
        "    sequence_length=encoder_seq_len, \n",
        "    dtype=tf.float32\n",
        ")\n",
        " \n",
        "del(encoder_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "Y2gkACBoXjuq",
        "outputId": "5c344d5c-cf18-4b0c-86c8-2d295db2cc7c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-53adf719dddc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Encoder embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m encoder_input_embedded = tf.compat.v1.estimator.layers.embed_sequence(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mencoder_input_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mINPUT_NUM_VOCAB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/module_wrapper.py\u001b[0m in \u001b[0;36m_getattr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \"\"\"\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m       \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_wrapped_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;31m# Placeholder for Google-internal contrib error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_estimator.python.estimator.api._v1.estimator' has no attribute 'layers'"
          ]
        }
      ]
    }
  ]
}