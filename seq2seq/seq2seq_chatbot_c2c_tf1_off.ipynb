{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HxcOYvrNVmB4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, re\n",
    "from tensorflow.python.layers.core import Dense\n",
    "import datetime\n",
    "\n",
    "# for bleu score\n",
    "# !pip install nltk\n",
    "from nltk.translate import bleu_score\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNMeYMsr6cfL",
    "outputId": "0aa3e329-c860-4d11-9d89-edb04cd6b0c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sIitZHrZVmB9"
   },
   "outputs": [],
   "source": [
    "MAX_CHAR_PER_LINE = 20\n",
    "\n",
    "def load_sentences(path):\n",
    "    with open(path, 'r', encoding=\"ISO-8859-1\") as f:\n",
    "        data_raw = f.read().encode('ascii', 'ignore').decode('UTF-8').lower()\n",
    "        data_alpha = re.sub('[^a-z\\n]+', ' ', data_raw)\n",
    "        data = []\n",
    "        for line in data_alpha.split('\\n'):\n",
    "            data.append(line[:MAX_CHAR_PER_LINE])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RKzlLMIHVmB-"
   },
   "outputs": [],
   "source": [
    "def extract_character_vocab(data):\n",
    "    special_symbols = ['<PAD>', '<UNK>', '<GO>',  '<EOS>']\n",
    "    set_symbols = set([character for line in data for character in line])\n",
    "    all_symbols = special_symbols + list(set_symbols)\n",
    "    int_to_symbol = {word_i: word for word_i, word in enumerate(all_symbols)}\n",
    "    symbol_to_int = {word: word_i for word_i, word in int_to_symbol.items()}\n",
    "    return int_to_symbol, symbol_to_int\n",
    "\n",
    "input_sentences = load_sentences('data/words_input.txt')  \n",
    "output_sentences = load_sentences('data/words_output.txt')  \n",
    "\n",
    "input_int_to_symbol, input_symbol_to_int = extract_character_vocab(input_sentences)\n",
    "output_int_to_symbol, output_symbol_to_int = extract_character_vocab(output_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lGt6mnoyVmB_",
    "outputId": "020245cd-3970-4277-a2f3-fc92a5074f47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<PAD>',\n",
       " 1: '<UNK>',\n",
       " 2: '<GO>',\n",
       " 3: '<EOS>',\n",
       " 4: 'o',\n",
       " 5: 'v',\n",
       " 6: 'm',\n",
       " 7: 'd',\n",
       " 8: 'l',\n",
       " 9: 'u',\n",
       " 10: 'a',\n",
       " 11: 'h',\n",
       " 12: 'j',\n",
       " 13: 'k',\n",
       " 14: 's',\n",
       " 15: 'r',\n",
       " 16: 'f',\n",
       " 17: 'i',\n",
       " 18: 'z',\n",
       " 19: 'c',\n",
       " 20: 't',\n",
       " 21: 'q',\n",
       " 22: 'y',\n",
       " 23: 'g',\n",
       " 24: 'b',\n",
       " 25: 'p',\n",
       " 26: 'x',\n",
       " 27: ' ',\n",
       " 28: 'n',\n",
       " 29: 'e',\n",
       " 30: 'w'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_int_to_symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-wNo8fyIVmCB",
    "outputId": "23e1c7f9-994d-479f-ae7f-13c9d060b3a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<PAD>',\n",
       " 1: '<UNK>',\n",
       " 2: '<GO>',\n",
       " 3: '<EOS>',\n",
       " 4: 'o',\n",
       " 5: 'v',\n",
       " 6: 'm',\n",
       " 7: 'd',\n",
       " 8: 'l',\n",
       " 9: 'u',\n",
       " 10: 'a',\n",
       " 11: 'h',\n",
       " 12: 'j',\n",
       " 13: 'k',\n",
       " 14: 's',\n",
       " 15: 'f',\n",
       " 16: 'r',\n",
       " 17: 'i',\n",
       " 18: 'z',\n",
       " 19: 'c',\n",
       " 20: 't',\n",
       " 21: 'q',\n",
       " 22: 'y',\n",
       " 23: 'g',\n",
       " 24: 'b',\n",
       " 25: 'p',\n",
       " 26: 'x',\n",
       " 27: ' ',\n",
       " 28: 'n',\n",
       " 29: 'e',\n",
       " 30: 'w'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_int_to_symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 50000\n"
     ]
    }
   ],
   "source": [
    "# split the data \n",
    "print(len(input_sentences), len(output_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n",
      "10 10 10\n",
      "you have known him l \n",
      " since his beard was \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    " \n",
    "\n",
    "# Shuffle two lists with same order\n",
    "# Using zip() + * operator + shuffle()\n",
    "temp = list(zip(input_sentences, output_sentences))\n",
    "random.shuffle(temp)\n",
    "res1, res2 = zip(*temp)\n",
    "# res1 and res2 come out as tuples, and so must be converted to lists.\n",
    "input_sentences, output_sentences = list(res1), list(res2)\n",
    "\n",
    "# split the data\n",
    "train_percent, evaluate_percent, test_percent = 0.7, 0.2, 0.1\n",
    "length_sentences = len(input_sentences)\n",
    "\n",
    "train_size = int(length_sentences*train_percent)\n",
    "\n",
    "train_input_sentences = input_sentences[:train_size]\n",
    "\n",
    "test_size = int(length_sentences*test_percent)\n",
    "\n",
    "test_input_sentences = input_sentences[-test_size:]\n",
    "\n",
    "evaluate_input_sentences = list(input_sentences[train_size:45000])\n",
    "\n",
    "\n",
    "\n",
    "train_output_sentences = output_sentences[:train_size]\n",
    "test_output_sentences = output_sentences[-test_size::]\n",
    "evaluate_output_sentences = list(input_sentences[train_size:45000])\n",
    "\n",
    "\n",
    "# for test ---- comment this part when finish the test --------------------------------\n",
    "size = 10\n",
    "train_input_sentences = train_input_sentences[:size]\n",
    "evaluate_input_sentences = evaluate_input_sentences[:size]\n",
    "test_input_sentences = test_input_sentences[:size]\n",
    "train_output_sentences = train_output_sentences[:size]\n",
    "evaluate_output_sentences = evaluate_output_sentences[:size]\n",
    "test_output_sentences = test_output_sentences[:size]\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "# Printing result\n",
    "print(len(train_input_sentences), len(evaluate_input_sentences), len(test_input_sentences))\n",
    "print(len(train_output_sentences), len(evaluate_output_sentences), len(test_output_sentences))\n",
    "\n",
    "print(train_input_sentences[1], '\\n', train_output_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XAhCn8_UVmCB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCS = 300\n",
    "RNN_STATE_DIM = 512\n",
    "RNN_NUM_LAYERS = 2\n",
    "ENCODER_EMBEDDING_DIM = DECODER_EMBEDDING_DIM = 64\n",
    " \n",
    "# BATCH_SIZE = int(32)\n",
    "\n",
    "# for test ----------------------------------------\n",
    "BATCH_SIZE = int(5)\n",
    "#--------------------------------------------------\n",
    "\n",
    "LEARNING_RATE = 0.0003\n",
    " \n",
    "INPUT_NUM_VOCAB = len(input_symbol_to_int)\n",
    "OUTPUT_NUM_VOCAB = len(output_symbol_to_int)\n",
    "print(OUTPUT_NUM_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "yKEIBCo-VmCC"
   },
   "outputs": [],
   "source": [
    "# Encoder placeholders\n",
    "encoder_input_seq = tf.placeholder(\n",
    "    tf.int32, \n",
    "    [None, None],  \n",
    "    name='encoder_input_seq'\n",
    ")\n",
    "\n",
    "encoder_seq_len = tf.placeholder(\n",
    "    tf.int32, \n",
    "    (None,), \n",
    "    name='encoder_seq_len'\n",
    ")\n",
    " \n",
    "# Decoder placeholders\n",
    "decoder_output_seq = tf.placeholder( \n",
    "    tf.int32, \n",
    "    [None, None],\n",
    "    name='decoder_output_seq'\n",
    ")\n",
    "\n",
    "decoder_seq_len = tf.placeholder(\n",
    "    tf.int32,\n",
    "    (None,), \n",
    "    name='decoder_seq_len'\n",
    ")\n",
    "\n",
    "max_decoder_seq_len = tf.reduce_max( \n",
    "    decoder_seq_len, \n",
    "    name='max_decoder_seq_len'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QGzM3RfvVmCE"
   },
   "outputs": [],
   "source": [
    "def make_cell(state_dim):\n",
    "    lstm_initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "    return tf.contrib.rnn.LSTMCell(state_dim, initializer=lstm_initializer)\n",
    " \n",
    "def make_multi_cell(state_dim, num_layers):\n",
    "    cells = [make_cell(state_dim) for _ in range(num_layers)]\n",
    "    return tf.contrib.rnn.MultiRNNCell(cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WOAWbAElVmCE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-11-b0e2ecdc0d3e>:3: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-11-b0e2ecdc0d3e>:7: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-12-daed38c1850a>:18: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000297F89C5DA0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000297F89C5DA0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000297F89C5DA0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000297F89C5DA0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:From D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FD3C2198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FD3C2198>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FD3C2198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FD3C2198>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FD3C2320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FD3C2320>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FD3C2320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FD3C2320>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:From D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Encoder embedding\n",
    " \n",
    "encoder_input_embedded = tf.contrib.layers.embed_sequence(\n",
    "    encoder_input_seq,     \n",
    "    INPUT_NUM_VOCAB,        \n",
    "    ENCODER_EMBEDDING_DIM  \n",
    ")\n",
    " \n",
    " \n",
    "# Encodering output\n",
    " \n",
    "encoder_multi_cell = make_multi_cell(RNN_STATE_DIM, RNN_NUM_LAYERS)\n",
    " \n",
    "encoder_output, encoder_state = tf.nn.dynamic_rnn(\n",
    "    encoder_multi_cell, \n",
    "    encoder_input_embedded, \n",
    "    sequence_length=encoder_seq_len, \n",
    "    dtype=tf.float32\n",
    ")\n",
    " \n",
    "del(encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2WVtIq8EVmCF"
   },
   "outputs": [],
   "source": [
    "decoder_raw_seq = decoder_output_seq[:, :-1]  \n",
    "go_prefixes = tf.fill([BATCH_SIZE, 1], output_symbol_to_int['<GO>'])  \n",
    "decoder_input_seq = tf.concat([go_prefixes, decoder_raw_seq], 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "H9X6Pr69VmCG"
   },
   "outputs": [],
   "source": [
    "decoder_embedding = tf.Variable(tf.random_uniform([OUTPUT_NUM_VOCAB, \n",
    "                                                   DECODER_EMBEDDING_DIM]))\n",
    "decoder_input_embedded = tf.nn.embedding_lookup(decoder_embedding, \n",
    "                                                decoder_input_seq)\n",
    "\n",
    "decoder_multi_cell = make_multi_cell(RNN_STATE_DIM, RNN_NUM_LAYERS)\n",
    " \n",
    "output_layer_kernel_initializer = tf.truncated_normal_initializer(mean=0.0, stddev=0.1)\n",
    "output_layer = Dense(\n",
    "    OUTPUT_NUM_VOCAB,\n",
    "    kernel_initializer = output_layer_kernel_initializer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Hzk43PUlVmCG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000297FEA55FD0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000297FEA55FD0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000297FEA55FD0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000297FEA55FD0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FEA55BE0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FEA55BE0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FEA55BE0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FEA55BE0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FEA55C50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FEA55C50>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FEA55C50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FEA55C50>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000297FEA55E80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000297FEA55E80>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000297FEA55E80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000297FEA55E80>>: AttributeError: module 'gast' has no attribute 'Index'\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"decode\"):\n",
    " \n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "        inputs=decoder_input_embedded,\n",
    "        sequence_length=decoder_seq_len,\n",
    "        time_major=False\n",
    "    )\n",
    " \n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        decoder_multi_cell,\n",
    "        training_helper,\n",
    "        encoder_state,\n",
    "        output_layer\n",
    "    ) \n",
    " \n",
    "    training_decoder_output_seq, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        training_decoder, \n",
    "        impute_finished=True, \n",
    "        maximum_iterations=max_decoder_seq_len\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Tm2WTYnlVmCH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000297FEA55FD0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000297FEA55FD0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000297FEA55FD0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000297FEA55FD0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FEA55BE0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FEA55BE0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FEA55BE0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FEA55BE0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FEA55C50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FEA55C50>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FEA55C50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000297FEA55C50>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000297FEA55E80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000297FEA55E80>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000297FEA55E80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000297FEA55E80>>: AttributeError: module 'gast' has no attribute 'Index'\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"decode\", reuse=True):\n",
    "    start_tokens = tf.tile(\n",
    "        tf.constant([output_symbol_to_int['<GO>']], \n",
    "                    dtype=tf.int32), \n",
    "        [BATCH_SIZE], \n",
    "        name='start_tokens')\n",
    " \n",
    "    # Helper for the inference process.\n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "        embedding=decoder_embedding,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=output_symbol_to_int['<EOS>']\n",
    "    )\n",
    " \n",
    "    # Basic decoder\n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        decoder_multi_cell,\n",
    "        inference_helper,\n",
    "        encoder_state,\n",
    "        output_layer\n",
    "    )\n",
    " \n",
    "    # Perform dynamic decoding using the decoder\n",
    "    inference_decoder_output_seq, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inference_decoder,\n",
    "        impute_finished=True,\n",
    "        maximum_iterations=40\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "YTti4wQjVmCI"
   },
   "outputs": [],
   "source": [
    "# rename the tensor for our convenience\n",
    "training_logits = tf.identity(training_decoder_output_seq.rnn_output, name='logits')\n",
    "inference_logits = tf.identity(inference_decoder_output_seq.sample_id, name='predictions')\n",
    "# inference_logits = tf.expand_dims(inference_decoder_output_seq.sample_id, axis=0)\n",
    " \n",
    "# Create the weights for sequence_loss\n",
    "masks = tf.sequence_mask(\n",
    "    decoder_seq_len, \n",
    "    max_decoder_seq_len, \n",
    "    dtype=tf.float32, \n",
    "    name='masks'\n",
    ")\n",
    " \n",
    "loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    training_logits,\n",
    "    decoder_output_seq,\n",
    "    masks\n",
    ")\n",
    "\n",
    "# Training summary for the current batch_loss\n",
    "tf.summary.scalar('loss', loss)\n",
    "summary_op = tf.summary.merge_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "__c0YVTDVmCI"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    " \n",
    "gradients = optimizer.compute_gradients(loss)\n",
    "capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var)\n",
    "                        for grad, var in gradients if grad is not None]\n",
    "train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "dMal2xbxVmCJ"
   },
   "outputs": [],
   "source": [
    "def pad(xs, size, pad):\n",
    "    return xs + [pad] * (size - len(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "EEhdzpYvVmCK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Process ...\n",
      "Epcoh 0. Batch 1/2. Loss 3.47695255279541\n",
      "Epcoh 0. Batch 2/2. Loss 2.9931163787841797\n",
      "\n",
      "Evaluation Process ...\n",
      "Epcoh 0. Batch 1/2. Loss 2.9721853733062744\n",
      "Epcoh 0. Batch 2/2. Loss 2.9771804809570312\n",
      "\n",
      "Test Process...\n",
      "[['f', 'u', 'c', 'k', ' ', 'i', 't', ' ', 'h', 'o', 'l', 'd', ' ', 't', 'i', 'g', 'h', 't', ' '], ['n', 'o', ' '], ['s', 'o', ' ', 't', 'e', 'l', 'l', ' ', 'm', 'e', ' ', 'm', 'o', 'r', 'e', ' ', 'a', 'b', 'o', 'u'], ['p', 'r', 'e', 't', 't', 'y', ' ', 'm', 'u', 'c', 'h', ' '], ['i', ' ', 'f', 'e', 'l', 'l', ' ', 'i', 'n', ' ', 'l', 'o', 'v', 'e', ' ', 'm', 'i', 'l', 'e', 's']]\n",
      "['fuck it hold tight ', 'no ', 'so tell me more abou', 'pretty much ', 'i fell in love miles']\n",
      "[[29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27], [29, 29, 29, 29, 29, 27, 29, 27, 27, 27, 27, 29, 29, 29, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27], [29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27], [10, 10, 27, 27, 14, 14, 14, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27], [29, 29, 29, 29, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27]]\n",
      "[['e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '], ['e', 'e', 'e', 'e', 'e', ' ', 'e', ' ', ' ', ' ', ' ', 'e', 'e', 'e', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '], ['e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '], ['a', 'a', ' ', ' ', 's', 's', 's', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '], ['e', 'e', 'e', 'e', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']]\n",
      "[['n', 'o', ' '], ['s', 'o', ' ', 't', 'e', 'l', 'l', ' ', 'm', 'e', ' ', 'm', 'o', 'r', 'e', ' ', 'a', 'b', 'o', 'u'], ['p', 'r', 'e', 't', 't', 'y', ' ', 'm', 'u', 'c', 'h', ' '], ['i', ' ', 'f', 'e', 'l', 'l', ' ', 'i', 'n', ' ', 'l', 'o', 'v', 'e', ' ', 'm', 'i', 'l', 'e', 's'], ['w', 'h', 'y', ' ']]\n",
      "['no ', 'so tell me more abou', 'pretty much ', 'i fell in love miles', 'why ']\n",
      "[[29, 29, 29, 29, 29, 27, 29, 27, 27, 27, 27, 29, 29, 29, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27], [29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27], [10, 10, 27, 27, 14, 14, 14, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27], [29, 29, 29, 29, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27], [20, 14, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27]]\n",
      "[['e', 'e', 'e', 'e', 'e', ' ', 'e', ' ', ' ', ' ', ' ', 'e', 'e', 'e', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '], ['e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '], ['a', 'a', ' ', ' ', 's', 's', 's', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '], ['e', 'e', 'e', 'e', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '], ['t', 's', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'decoder_output_seq' with dtype int32 and shape [?,?]\n\t [[node decoder_output_seq (defined at <ipython-input-10-26c7cc45c629>:18) ]]\n\nOriginal stack trace for 'decoder_output_seq':\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 619, in start\n    self.io_loop.start()\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\asyncio\\base_events.py\", line 442, in run_forever\n    self._run_once()\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\asyncio\\base_events.py\", line 1462, in _run_once\n    handle._run()\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n    ret = callback()\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n    self.ctx_run(self.run)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 377, in dispatch_queue\n    yield self.process_one()\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 250, in wrapper\n    runner = Runner(ctx_run, result, future, yielded)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 741, in __init__\n    self.ctx_run(self.run)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 261, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 541, in execute_request\n    user_expressions, allow_stdin,\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 302, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 539, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2867, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2895, in _run_cell\n    return runner(coro)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3072, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3263, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-26c7cc45c629>\", line 18, in <module>\n    name='decoder_output_seq'\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2143, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 7400, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mD:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'decoder_output_seq' with dtype int32 and shape [?,?]\n\t [[{{node decoder_output_seq}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-83025c1ec3ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[0mbleu_summary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bleu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbleu_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[0mall_bleu_summary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m     \u001b[0mbleu_summary_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_bleu_summary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;31m# #     saver.save(sess, 'model.ckpt')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1368\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'decoder_output_seq' with dtype int32 and shape [?,?]\n\t [[node decoder_output_seq (defined at <ipython-input-10-26c7cc45c629>:18) ]]\n\nOriginal stack trace for 'decoder_output_seq':\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 619, in start\n    self.io_loop.start()\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\asyncio\\base_events.py\", line 442, in run_forever\n    self._run_once()\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\asyncio\\base_events.py\", line 1462, in _run_once\n    handle._run()\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n    ret = callback()\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n    self.ctx_run(self.run)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 377, in dispatch_queue\n    yield self.process_one()\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 250, in wrapper\n    runner = Runner(ctx_run, result, future, yielded)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 741, in __init__\n    self.ctx_run(self.run)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 261, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 541, in execute_request\n    user_expressions, allow_stdin,\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tornado\\gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 302, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 539, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2867, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2895, in _run_cell\n    return runner(coro)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3072, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3263, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-26c7cc45c629>\", line 18, in <module>\n    name='decoder_output_seq'\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2143, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 7400, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "input_seq = [\n",
    "    [input_symbol_to_int.get(symbol, input_symbol_to_int['<UNK>']) \n",
    "        for symbol in line]  \n",
    "    for line in input_sentences  \n",
    "]\n",
    " \n",
    "output_seq = [\n",
    "    [output_symbol_to_int.get(symbol, output_symbol_to_int['<UNK>']) \n",
    "        for symbol in line] + [output_symbol_to_int['<EOS>']]  \n",
    "    for line in output_sentences  \n",
    "]\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()  \n",
    " \n",
    "\n",
    "# step_per_checkpoint = 629\n",
    "\n",
    "# test ----------------------------------------------------------------\n",
    "step_per_checkpoint = 1\n",
    "BATCH_SIZE = 5\n",
    "NUM_EPOCS = 2\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/seq2seq/' + current_time + '/train'\n",
    "evaluation_log_dir = 'logs/seq2seq/' + current_time + '/evaluation'\n",
    "bleu_test_log_dir = 'logs/seq2seq/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.FileWriter(train_log_dir, graph=sess.graph)\n",
    "evaluation_summary_writer = tf.summary.FileWriter(evaluation_log_dir, graph=sess.graph)\n",
    "bleu_summary_writer = tf.summary.FileWriter(bleu_test_log_dir, graph=sess.graph)\n",
    "\n",
    "for epoch in range(NUM_EPOCS + 1): \n",
    "\n",
    "    current_train_step = 0\n",
    "    current_evaluate_step = 0\n",
    "    # train\n",
    "    print('Training Process ...')\n",
    "    for batch_idx in range(len(train_input_sentences) // BATCH_SIZE): \n",
    "        \n",
    "        input_batch, input_lengths, output_batch, output_lengths = [], [], [], []\n",
    "        for sentence in train_input_sentences[batch_idx:batch_idx + BATCH_SIZE]:\n",
    "            symbol_sent = [input_symbol_to_int[symbol] for symbol in sentence]\n",
    "            padded_symbol_sent = pad(symbol_sent, MAX_CHAR_PER_LINE, input_symbol_to_int['<PAD>'])\n",
    "            input_batch.append(padded_symbol_sent)\n",
    "            input_lengths.append(len(sentence))\n",
    "        for sentence in train_output_sentences[batch_idx:batch_idx + BATCH_SIZE]:\n",
    "            symbol_sent = [output_symbol_to_int[symbol] for symbol in sentence]\n",
    "            padded_symbol_sent = pad(symbol_sent, MAX_CHAR_PER_LINE, output_symbol_to_int['<PAD>'])\n",
    "            output_batch.append(padded_symbol_sent)\n",
    "            output_lengths.append(len(sentence))\n",
    "        \n",
    "        \n",
    "        current_train_step += 1\n",
    "        \n",
    "        _, train_loss, train_summary = sess.run( \n",
    "            [train_op, loss, summary_op],\n",
    "            feed_dict={\n",
    "                encoder_input_seq: input_batch,\n",
    "                encoder_seq_len: input_lengths,\n",
    "                decoder_output_seq: output_batch,\n",
    "                decoder_seq_len: output_lengths\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if current_train_step % step_per_checkpoint == 0:\n",
    "            print('Epcoh {}. Batch {}/{}. Loss {}'.format(epoch, current_train_step, len(train_input_sentences) // BATCH_SIZE, train_loss))\n",
    "    \n",
    "    train_summary_writer.add_summary(train_summary, epoch)\n",
    "        \n",
    "    # evaluate \n",
    "    print('\\nEvaluation Process ...')\n",
    "    \n",
    "    for batch_idx in range(len(evaluate_input_sentences) // BATCH_SIZE): \n",
    "        \n",
    "        input_batch, input_lengths, output_batch, output_lengths = [], [], [], []\n",
    "        for sentence in evaluate_input_sentences[batch_idx:batch_idx + BATCH_SIZE]:\n",
    "            symbol_sent = [input_symbol_to_int[symbol] for symbol in sentence]\n",
    "            padded_symbol_sent = pad(symbol_sent, MAX_CHAR_PER_LINE, input_symbol_to_int['<PAD>'])\n",
    "            input_batch.append(padded_symbol_sent)\n",
    "            input_lengths.append(len(sentence))\n",
    "        for sentence in evaluate_output_sentences[batch_idx:batch_idx + BATCH_SIZE]:\n",
    "            symbol_sent = [output_symbol_to_int[symbol] for symbol in sentence]\n",
    "            padded_symbol_sent = pad(symbol_sent, MAX_CHAR_PER_LINE, output_symbol_to_int['<PAD>'])\n",
    "            output_batch.append(padded_symbol_sent)\n",
    "            output_lengths.append(len(sentence))\n",
    "        \n",
    "        current_evaluate_step += 1\n",
    "        \n",
    "        evaluation_loss, evaluate_summary, logits = sess.run( \n",
    "            [loss, summary_op, inference_logits],\n",
    "            feed_dict={\n",
    "                encoder_input_seq: input_batch,\n",
    "                encoder_seq_len: input_lengths,\n",
    "                decoder_output_seq: output_batch,\n",
    "                decoder_seq_len: output_lengths\n",
    "            }\n",
    "        )\n",
    "        \n",
    "#         print(logits.shape)\n",
    "            \n",
    "        \n",
    "        if current_evaluate_step % step_per_checkpoint == 0:\n",
    "            print('Epcoh {}. Batch {}/{}. Loss {}'.format(epoch, current_evaluate_step, len(evaluate_input_sentences) // BATCH_SIZE, evaluation_loss))\n",
    "    \n",
    "    evaluation_summary_writer.add_summary(evaluate_summary, epoch)\n",
    "    \n",
    "    # test \n",
    "    print('\\nTest Process...')\n",
    "\n",
    "    bleu_epoch = 0\n",
    "    bleu_val = 0\n",
    "    for batch_idx in range(len(test_input_sentences) // BATCH_SIZE): \n",
    "        \n",
    "        input_batch, input_lengths, output_voc_seq, output_voc_lengths = [], [], [], []\n",
    "        for i, sentence in enumerate(test_input_sentences[batch_idx:batch_idx + BATCH_SIZE]):\n",
    "            symbol_sent = [input_symbol_to_int[symbol] for symbol in sentence]\n",
    "            padded_symbol_sent = pad(symbol_sent, MAX_CHAR_PER_LINE, input_symbol_to_int['<PAD>'])\n",
    "            input_batch.append(padded_symbol_sent)\n",
    "            input_lengths.append(len(sentence))\n",
    "\n",
    "\n",
    "        true_char_seq = []\n",
    "        test_ls = []\n",
    "        for sentence in test_output_sentences[batch_idx:batch_idx + BATCH_SIZE]:\n",
    "            test_ls.append(sentence)\n",
    "            char_ls = []\n",
    "            for char in sentence:\n",
    "                char_ls.append(char)\n",
    "            true_char_seq.append(char_ls)\n",
    "        print(true_char_seq)\n",
    "        print(test_ls)\n",
    "#         break\n",
    "\n",
    "        \n",
    "        predict_seq = sess.run( \n",
    "            inference_logits,\n",
    "            feed_dict={\n",
    "                encoder_input_seq: input_batch,\n",
    "                encoder_seq_len: input_lengths,\n",
    "\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    \n",
    "        predict_list = np.ndarray.tolist(predict_seq)\n",
    "        print(predict_list)\n",
    "    \n",
    "        predict_char_seq = []\n",
    "        for i in range(len(predict_list)):\n",
    "            temp = []\n",
    "            for idx in predict_list[i]:\n",
    "                temp.append(output_int_to_symbol[idx])\n",
    "            predict_char_seq.append(temp)\n",
    "        \n",
    "        print(predict_char_seq)\n",
    "        \n",
    "        sm_fn = SmoothingFunction()\n",
    "        bleu_val += bleu_score.corpus_bleu(true_char_seq[:40], predict_char_seq, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=sm_fn.method1)\n",
    "    \n",
    "    bleu_epoch = bleu_val\n",
    "    bleu_summary = tf.summary.scalar('bleu', bleu_epoch)\n",
    "    bleu_summary_writer.add_summary(sess.run(bleu_summary), epoch)\n",
    "        \n",
    "# #     saver.save(sess, 'model.ckpt')   \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 16144), started 0:01:16 ago. (Use '!kill 16144' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x297fea2ff28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "tefSsBcQVmCL",
    "outputId": "add8b911-dded-4800-cdd3-cca85150e2b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The passed save_path is not a valid checkpoint: model.ckpt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-8e925a12b4dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'model.ckpt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mexample_input_sent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"do you want to play games\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mexample_input_symb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minput_symbol_to_int\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msymbol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msymbol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexample_input_sent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\miniconda3\\envs\\seq2seq_tf1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheckpoint_management\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckpoint_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m       raise ValueError(\"The passed save_path is not a valid checkpoint: \" +\n\u001b[1;32m-> 1278\u001b[1;33m                        compat.as_text(save_path))\n\u001b[0m\u001b[0;32m   1279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Restoring parameters from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The passed save_path is not a valid checkpoint: model.ckpt"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()    \n",
    "saver.restore(sess, 'model.ckpt')\n",
    "\n",
    "example_input_sent = \"do you want to play games\"\n",
    "example_input_symb = [input_symbol_to_int[symbol] for symbol in example_input_sent]\n",
    "example_input_batch = [pad(example_input_symb, MAX_CHAR_PER_LINE, input_symbol_to_int['<PAD>'])] * BATCH_SIZE\n",
    "example_input_lengths = [len(example_input_sent)] * BATCH_SIZE\n",
    "\n",
    "output_ints = sess.run(inference_logits, feed_dict={\n",
    "    encoder_input_seq: example_input_batch,\n",
    "    encoder_seq_len: example_input_lengths,\n",
    "    decoder_seq_len: example_input_lengths\n",
    "})[0]\n",
    "\n",
    "output_str = ''.join([output_int_to_symbol[i] for i in output_ints])\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "a[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "testkernel",
   "language": "python",
   "name": "testkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
